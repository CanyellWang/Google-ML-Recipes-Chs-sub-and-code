1
00:00:00,000 --> 00:00:00,000
Youtube subtitles download by mo.dbxdb.com 
翻译 yyn19951228

2
00:00:00,000 --> 00:00:02,844
[MUSIC PLAYING]
[音乐]

3
00:00:02,844 --> 00:00:06,640


4
00:00:06,640 --> 00:00:07,447
Welcome back.
欢迎回来

5
00:00:07,447 --> 00:00:09,029
We've covered a lot
of ground already,
我们已经学习了很多基础知识

6
00:00:09,029 --> 00:00:12,070
so today I want to review
and reinforce concepts.
所以今天我想再复习加强一下基础概念

7
00:00:12,070 --> 00:00:14,250
To do that, we'll
explore two things.
我们通过两个例子作为开始

8
00:00:14,250 --> 00:00:16,090
First, we'll code
up a basic pipeline
首先，我们为监督学习

9
00:00:16,090 --> 00:00:17,640
for supervised learning.
来码一个基础的管道封装

10
00:00:17,640 --> 00:00:19,390
I'll show you how
multiple classifiers
我将会向你们展示多种分类器是如何

11
00:00:19,390 --> 00:00:21,280
can solve the same problem.
能够解决同样问题的。

12
00:00:21,280 --> 00:00:23,200
Next, we'll build up a
little more intuition
之后，我们会获得一些对于

13
00:00:23,200 --> 00:00:25,710
for what it means for an
algorithm to learn something
从数据当中建立模型和算法的启发。

14
00:00:25,710 --> 00:00:29,502
from data, because that sounds
kind of magical, but it's not.
虽然看起来很神奇，但是完全不需要担心。

15
00:00:29,502 --> 00:00:31,710
To kick things off, let's
look at a common experiment
作为开始，让我们来看一个很普通的，

16
00:00:31,710 --> 00:00:33,009
you might want to do.
你们也许很想做的实验。

17
00:00:33,009 --> 00:00:35,210
Imagine you're building
a spam classifier.
想象一下你正在垃圾邮件分类器。

18
00:00:35,210 --> 00:00:37,510
That's just a function that
labels an incoming email
这是一个对邮件加标签的功能，

19
00:00:37,510 --> 00:00:39,307
as spam or not spam.
把邮件标为垃圾邮件或者非垃圾邮件。

20
00:00:39,307 --> 00:00:41,140
Now, say you've already
collected a data set
现在，假设你已经收集了一些数据集

21
00:00:41,140 --> 00:00:42,850
and you're ready
to train a model.
而且你准备训练一个模型。

22
00:00:42,850 --> 00:00:44,460
But before you put
it into production,
但是在你把这个模型投入到实际应用之前，

23
00:00:44,460 --> 00:00:46,760
there's a question you
need to answer first--
需要首先关注一个问题！

24
00:00:46,760 --> 00:00:49,820
how accurate will it be when you
use it to classify emails that
当分类的邮件不在你收集到的数据集当中的时候，

25
00:00:49,820 --> 00:00:51,740
weren't in your training data?
怎样保证你的模型预测是准确的？

26
00:00:51,740 --> 00:00:54,850
As best we can, we want to
verify our models work well
为了能做到最好，我们希望能够在投入实际应用前

27
00:00:54,850 --> 00:00:56,490
before we deploy them.
确认我们的模型有最好的泛化能力。

28
00:00:56,490 --> 00:00:59,290
And we can do an experiment
to help us figure that out.
我们可以做一个实验来帮助我们确认。

29
00:00:59,290 --> 00:01:02,930
One approach is to partition
our data set into two parts.
我们的方法是把我们收集到的数据集一分为二。

30
00:01:02,930 --> 00:01:05,079
We'll call these Train and Test.
我们将它们分别称为“训练集”和“测试集”

31
00:01:05,079 --> 00:01:07,010
We'll use Train
to train our model
我们将采用“训练集”去训练我们的模型。

32
00:01:07,010 --> 00:01:10,380
and Test to see how
accurate it is on new data.
然后用“测试集”来测试模型的泛化准确性。

33
00:01:10,380 --> 00:01:13,890
That's a common pattern, so
let's see how it looks in code.
这是一个很常见的套路，所以让我们来看看
如何用代码来实现。

34
00:01:13,890 --> 00:01:17,060
To kick things off, let's import
a data set into [? SyKit. ?]
作为开始，让我们import一个数据集到[? SyKit. ?]中

35
00:01:17,060 --> 00:01:20,019
We'll use Iris again, because
it's handily included.
我们将再一次的使用“鸢尾花”数据集，因为
这些使用十分方便。

36
00:01:20,019 --> 00:01:21,959
Now, we already saw
Iris in episode two.
现在，我们已经在第二章看过“鸢尾花”数据的表现了

37
00:01:21,959 --> 00:01:23,560
But what we haven't
seen before is
但是我们之前没有看过的是，

38
00:01:23,560 --> 00:01:26,831
that I'm calling the
features x and the labels y.
特征x 和标签y

39
00:01:26,831 --> 00:01:28,209
Why is that?
为什么是这个？

40
00:01:28,209 --> 00:01:30,670
Well, that's because one
way to think of a classifier

41
00:01:30,670 --> 00:01:32,230
is as a function.

42
00:01:32,230 --> 00:01:34,750
At a high level, you can
think of x as the input

43
00:01:34,750 --> 00:01:36,500
and y as the output.

44
00:01:36,500 --> 00:01:39,892
I'll talk more about that in
the second half of this episode.

45
00:01:39,892 --> 00:01:42,349
After we import the data set,
the first thing we want to do

46
00:01:42,349 --> 00:01:44,590
is partition it
into Train and Test.

47
00:01:44,590 --> 00:01:46,640
And to do that, we can
import a handy utility,

48
00:01:46,640 --> 00:01:48,530
and it makes the syntax clear.

49
00:01:48,530 --> 00:01:50,340
We're taking our
x's and our y's,

50
00:01:50,340 --> 00:01:52,930
or our features and labels,
and partitioning them

51
00:01:52,930 --> 00:01:54,450
into two sets.

52
00:01:54,450 --> 00:01:56,690
X_train and y_train are
the features and labels

53
00:01:56,690 --> 00:01:57,980
for the training set.

54
00:01:57,980 --> 00:02:00,630
And X_test and y_test are
the features and labels

55
00:02:00,630 --> 00:02:02,031
for the testing set.

56
00:02:02,031 --> 00:02:04,239
Here, I'm just saying that
I want half the data to be

57
00:02:04,239 --> 00:02:05,580
used for testing.

58
00:02:05,580 --> 00:02:09,229
So if we have 150 examples
in Iris, 75 will be in Train

59
00:02:09,229 --> 00:02:11,520
and 75 will be in Test.

60
00:02:11,520 --> 00:02:13,280
Now we'll create our classifier.

61
00:02:13,280 --> 00:02:14,979
I'll use two
different types here

62
00:02:14,979 --> 00:02:17,860
to show you how they
accomplish the same task.

63
00:02:17,860 --> 00:02:20,500
Let's start with the decision
tree we've already seen.

64
00:02:20,500 --> 00:02:22,240
Note there's only
two lines of code

65
00:02:22,240 --> 00:02:23,448
that are classifier-specific.

66
00:02:23,448 --> 00:02:25,650


67
00:02:25,650 --> 00:02:28,830
Now let's train the classifier
using our training data.

68
00:02:28,830 --> 00:02:31,599
At this point, it's ready
to be used to classify data.

69
00:02:31,599 --> 00:02:33,330
And next, we'll call
the predict method

70
00:02:33,330 --> 00:02:35,805
and use it to classify
our testing data.

71
00:02:35,805 --> 00:02:37,180
If you print out
the predictions,

72
00:02:37,180 --> 00:02:38,970
you'll see there are
a list of numbers.

73
00:02:38,970 --> 00:02:40,660
These correspond
to the type of Iris

74
00:02:40,660 --> 00:02:44,009
the classifier predicts for
each row in the testing data.

75
00:02:44,009 --> 00:02:46,229
Now let's see how
accurate our classifier

76
00:02:46,229 --> 00:02:48,280
was on the testing set.

77
00:02:48,280 --> 00:02:50,840
Recall that up top, we have
the true labels for the testing

78
00:02:50,840 --> 00:02:51,650
data.

79
00:02:51,650 --> 00:02:53,460
To calculate our
accuracy, we can

80
00:02:53,460 --> 00:02:55,759
compare the predicted
labels to the true labels,

81
00:02:55,759 --> 00:02:57,348
and tally up the score.

82
00:02:57,348 --> 00:02:59,139
There's a convenience
method in [? Sykit ?]

83
00:02:59,139 --> 00:03:00,830
we can import to do that.

84
00:03:00,830 --> 00:03:03,505
Notice here, our
accuracy was over 90%.

85
00:03:03,505 --> 00:03:06,130
If you try this on your own, it
might be a little bit different

86
00:03:06,130 --> 00:03:08,270
because of some randomness
in how the Train/Test

87
00:03:08,270 --> 00:03:10,039
data is partitioned.

88
00:03:10,039 --> 00:03:11,880
Now, here's something
interesting.

89
00:03:11,880 --> 00:03:14,690
By replacing these two lines, we
can use a different classifier

90
00:03:14,690 --> 00:03:16,919
to accomplish the same task.

91
00:03:16,919 --> 00:03:18,569
Instead of using
a decision tree,

92
00:03:18,569 --> 00:03:20,930
we'll use one called
[? KNearestNeighbors. ?]

93
00:03:20,930 --> 00:03:23,340
If we run our experiment,
we'll see that the code

94
00:03:23,340 --> 00:03:25,354
works in exactly the same way.

95
00:03:25,354 --> 00:03:27,270
The accuracy may be
different when you run it,

96
00:03:27,270 --> 00:03:29,800
because this classifier works
a little bit differently

97
00:03:29,800 --> 00:03:32,440
and because of the randomness
in the Train/Test split.

98
00:03:32,440 --> 00:03:35,419
Likewise, if we wanted to use a
more sophisticated classifier,

99
00:03:35,419 --> 00:03:38,220
we could just import it
and change these two lines.

100
00:03:38,220 --> 00:03:40,297
Otherwise, our code is the same.

101
00:03:40,297 --> 00:03:42,880
The takeaway here is that while
there are many different types

102
00:03:42,880 --> 00:03:45,919
of classifiers, at a high level,
they have a similar interface.

103
00:03:45,919 --> 00:03:49,058


104
00:03:49,058 --> 00:03:50,849
Now let's talk a little
bit more about what

105
00:03:50,849 --> 00:03:53,120
it means to learn from data.

106
00:03:53,120 --> 00:03:56,080
Earlier, I said we called the
features x and the labels y,

107
00:03:56,080 --> 00:03:58,717
because they were the input
and output of a function.

108
00:03:58,717 --> 00:04:00,800
Now, of course, a function
is something we already

109
00:04:00,800 --> 00:04:02,190
know from programming.

110
00:04:02,190 --> 00:04:04,900
def classify--
there's our function.

111
00:04:04,900 --> 00:04:06,919
As we already know in
supervised learning,

112
00:04:06,919 --> 00:04:09,060
we don't want to
write this ourselves.

113
00:04:09,060 --> 00:04:12,360
We want an algorithm to
learn it from training data.

114
00:04:12,360 --> 00:04:15,240
So what does it mean
to learn a function?

115
00:04:15,240 --> 00:04:17,120
Well, a function is just
a mapping from input

116
00:04:17,120 --> 00:04:18,660
to output values.

117
00:04:18,660 --> 00:04:20,660
Here's a function you
might have seen before-- y

118
00:04:20,660 --> 00:04:22,699
equals mx plus b.

119
00:04:22,699 --> 00:04:24,819
That's the equation
for a line, and there

120
00:04:24,819 --> 00:04:27,339
are two parameters-- m,
which gives the slope;

121
00:04:27,339 --> 00:04:29,680
and b, which gives
the y-intercept.

122
00:04:29,680 --> 00:04:31,110
Given these
parameters, of course,

123
00:04:31,110 --> 00:04:34,319
we can plot the function
for different values of x.

124
00:04:34,319 --> 00:04:36,610
Now, in supervised learning,
our classified function

125
00:04:36,610 --> 00:04:38,420
might have some
parameters as well,

126
00:04:38,420 --> 00:04:41,290
but the input x are the
features for an example we

127
00:04:41,290 --> 00:04:43,630
want to classify,
and the output y

128
00:04:43,630 --> 00:04:47,220
is a label, like Spam or Not
Spam, or a type of flower.

129
00:04:47,220 --> 00:04:49,661
So what could the body of
the function look like?

130
00:04:49,661 --> 00:04:51,910
Well, that's the part we
want to write algorithmically

131
00:04:51,910 --> 00:04:53,737
or in other words, learn.

132
00:04:53,737 --> 00:04:55,319
The important thing
to understand here

133
00:04:55,319 --> 00:04:57,130
is we're not
starting from scratch

134
00:04:57,130 --> 00:05:00,060
and pulling the body of the
function out of thin air.

135
00:05:00,060 --> 00:05:01,990
Instead, we start with a model.

136
00:05:01,990 --> 00:05:04,050
And you can think of a
model as the prototype for

137
00:05:04,050 --> 00:05:07,029
or the rules that define
the body of our function.

138
00:05:07,029 --> 00:05:08,540
Typically, a model
has parameters

139
00:05:08,540 --> 00:05:10,290
that we can adjust
with our training data.

140
00:05:10,290 --> 00:05:14,560
And here's a high-level example
of how this process works.

141
00:05:14,560 --> 00:05:17,380
Let's look at a toy data set and
think about what kind of model

142
00:05:17,380 --> 00:05:19,209
we could use as a classifier.

143
00:05:19,209 --> 00:05:20,959
Pretend we're interested
in distinguishing

144
00:05:20,959 --> 00:05:23,350
between red dots and
green dots, some of which

145
00:05:23,350 --> 00:05:25,079
I've drawn here on a graph.

146
00:05:25,079 --> 00:05:27,209
To do that, we'll use
just two features--

147
00:05:27,209 --> 00:05:29,449
the x- and
y-coordinates of a dot.

148
00:05:29,449 --> 00:05:32,670
Now let's think about how
we could classify this data.

149
00:05:32,670 --> 00:05:34,089
We want a function
that considers

150
00:05:34,089 --> 00:05:35,800
a new dot it's
never seen before,

151
00:05:35,800 --> 00:05:38,170
and classifies it
as red or green.

152
00:05:38,170 --> 00:05:40,990
In fact, there might be a lot
of data we want to classify.

153
00:05:40,990 --> 00:05:42,839
Here, I've drawn
our testing examples

154
00:05:42,839 --> 00:05:44,959
in light green and light red.

155
00:05:44,959 --> 00:05:47,209
These are dots that weren't
in our training data.

156
00:05:47,209 --> 00:05:49,790
The classifier has never
seen them before, so how can

157
00:05:49,790 --> 00:05:51,699
it predict the right label?

158
00:05:51,699 --> 00:05:53,819
Well, imagine if we
could somehow draw a line

159
00:05:53,819 --> 00:05:56,036
across the data like this.

160
00:05:56,036 --> 00:05:57,620
Then we could say
the dots to the left

161
00:05:57,620 --> 00:06:00,089
of the line are green and dots
to the right of the line are

162
00:06:00,089 --> 00:06:00,089
red.

163
00:06:00,920 --> 00:06:03,430
And this line can serve
as our classifier.

164
00:06:03,430 --> 00:06:05,610
So how can we learn this line?

165
00:06:05,610 --> 00:06:08,240
Well, one way is to use
the training data to adjust

166
00:06:08,240 --> 00:06:09,880
the parameters of a model.

167
00:06:09,880 --> 00:06:12,829
And let's say the model we
use is a simple straight line

168
00:06:12,829 --> 00:06:14,459
like we saw before.

169
00:06:14,459 --> 00:06:17,829
That means we have two
parameters to adjust-- m and b.

170
00:06:17,829 --> 00:06:21,050
And by changing them, we can
change where the line appears.

171
00:06:21,050 --> 00:06:23,500
So how could we learn
the right parameters?

172
00:06:23,500 --> 00:06:25,690
Well, one idea is that
we can iteratively adjust

173
00:06:25,690 --> 00:06:27,639
them using our training data.

174
00:06:27,639 --> 00:06:29,889
For example, we might
start with a random line

175
00:06:29,889 --> 00:06:32,810
and use it to classify the
first training example.

176
00:06:32,810 --> 00:06:35,370
If it gets it right, we don't
need to change our line,

177
00:06:35,370 --> 00:06:36,968
so we move on to the next one.

178
00:06:36,968 --> 00:06:38,759
But on the other hand,
if it gets it wrong,

179
00:06:38,759 --> 00:06:41,300
we could slightly adjust
the parameters of our model

180
00:06:41,300 --> 00:06:43,069
to make it more accurate.

181
00:06:43,069 --> 00:06:44,680
The takeaway here is this.

182
00:06:44,680 --> 00:06:47,490
One way to think of learning
is using training data

183
00:06:47,490 --> 00:06:50,980
to adjust the
parameters of a model.

184
00:06:50,980 --> 00:06:52,949
Now, here's something
really special.

185
00:06:52,949 --> 00:06:55,269
It's called
tensorflow/playground.

186
00:06:55,269 --> 00:06:57,370
This is a beautiful
example of a neural network

187
00:06:57,370 --> 00:07:00,019
you can run and experiment
with right in your browser.

188
00:07:00,019 --> 00:07:02,060
Now, this deserves its
own episode for sure,

189
00:07:02,060 --> 00:07:03,730
but for now, go ahead
and play with it.

190
00:07:03,730 --> 00:07:04,930
It's awesome.

191
00:07:04,930 --> 00:07:06,630
The playground comes
with different data

192
00:07:06,630 --> 00:07:08,300
sets you can try out.

193
00:07:08,300 --> 00:07:09,470
Some are very simple.

194
00:07:09,470 --> 00:07:12,620
For example, we could use our
line to classify this one.

195
00:07:12,620 --> 00:07:15,980
Some data sets are
much more complex.

196
00:07:15,980 --> 00:07:17,620
This data set is
especially hard.

197
00:07:17,620 --> 00:07:20,357
And see if you can build
a network to classify it.

198
00:07:20,357 --> 00:07:21,940
Now, you can think
of a neural network

199
00:07:21,940 --> 00:07:24,170
as a more sophisticated
type of classifier,

200
00:07:24,170 --> 00:07:26,430
like a decision tree
or a simple line.

201
00:07:26,430 --> 00:07:29,190
But in principle,
the idea is similar.

202
00:07:29,190 --> 00:07:29,190
OK.

203
00:07:29,690 --> 00:07:30,687
Hope that was helpful.

204
00:07:30,687 --> 00:07:32,519
I just created a Twitter
that you can follow

205
00:07:32,519 --> 00:07:33,834
to be notified of new episodes.

206
00:07:33,834 --> 00:07:36,000
And the next one should be
out in a couple of weeks,

207
00:07:36,000 --> 00:07:38,750
depending on how much work I'm
doing for Google I/O. Thanks,

208
00:07:38,750 --> 00:07:41,620
as always, for watching,
and I'll see you next time.

209
00:07:41,620 --> 00:07:53,000
 Subtitles End: mo.dbxdb.com

